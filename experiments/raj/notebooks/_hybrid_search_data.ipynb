{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dai/anaconda3/envs/ragenv/lib/python3.11/site-packages/weaviate/__init__.py:128: DeprecationWarning: Dep010: Importing AuthApiKey from weaviate is deprecated. Please import it from its specific module: weaviate.auth\n",
      "  _Warnings.root_module_import(name, map_[name])\n",
      "/home/dai/anaconda3/envs/ragenv/lib/python3.11/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "\n",
    "from src.indexing_utils import load_docs\n",
    "\n",
    "Settings.embed_model = GeminiEmbedding(model_name='models/embedding-001', task_type=\"retrieval_query\")\n",
    "Settings.llm = Gemini(model_name='models/gemini-pro', temperature=0.5)\n",
    "\n",
    "# Connect to local instance\n",
    "client = weaviate.Client(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = load_docs(path=\"/home/dai/33/project/rag/storage/docstore/article_nodes_embedded_combined\",\n",
    "                  return_docstore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"LlamaIndex_articles\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, that is correct. The document you provided does not mention anything about Nidaana Parivarjana. Therefore, I cannot provide an answer to your question based on the provided document."
     ]
    }
   ],
   "source": [
    "# Query Index with Hybrid Search as chat engine\n",
    "chat_engine = index.as_chat_engine(\n",
    "    vector_store_query_mode=\"hybrid\", similarity_top_k=2, alpha=0.8, chat_mode=\"condense_plus_context\"\n",
    ")\n",
    "streaming_response = chat_engine.chat(\"What is Nidaana Parivarjana?\")\n",
    "for token in streaming_response.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
